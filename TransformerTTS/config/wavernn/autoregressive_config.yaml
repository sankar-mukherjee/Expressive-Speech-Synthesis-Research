# use speaker embeddings
use_speaker_style_tts: True  # change for tts with only style / style + speaker
# use style loss
use_style_loss: False  # use style encoder 2nd time with decoder output as input and l2 between 1st and 2nd style embeddings
# use_mine
use_mine: True
# pretrain
use_pretrained_text_encoder: True
train_text_encoder: True
train_style_encoder: True
train_decoder: True

# TTS ARCHITECTURE
decoder_model_dimension: 256
encoder_model_dimension: 256
decoder_num_heads: [4, 4, 4, 4]  # the length of this defines the number of layers
encoder_num_heads: [4, 4, 4, 4]  # the length of this defines the number of layers
encoder_feed_forward_dimension: 1024
decoder_feed_forward_dimension: 1024
decoder_prenet_dimension: 256
encoder_prenet_dimension: 256
encoder_attention_conv_filters: 256
decoder_attention_conv_filters: 256
encoder_attention_conv_kernel: 3
decoder_attention_conv_kernel: 3
encoder_max_position_encoding: 1000
decoder_max_position_encoding: 10000
postnet_conv_filters: 256
postnet_conv_layers: 5
postnet_kernel_size: 5
encoder_dense_blocks: 4
decoder_dense_blocks: 4

# REFERENCE ENCODER & GST ARCHITECTURE
ref_encoder_filters: [32, 32, 64, 64, 128, 128]
ref_encoder_kernel_size: 3
ref_encoder_strides: 2
ref_encoder_gru_cell_units: 128
gst_style_embed_dim: 256
gst_multi_num_heads: 4
gst_heads: 10

# MINE for Mutual Information (MI)
use_club: True
mine_no_of_nets: 3
mine_pair_types: ['style_text','style_speaker','text_speaker'] # ['style_text_speaker'] #
divergence_type: 'KL'     # KL, reyni
mine_beta_values: [0, 0.5, 1]
mine_conv_filters: [2]
mine_conv_kernel: 5
mine_dense_hidden_units: [512, 64]
learning_rate_mine_schedule:
  - [1.0e-5, 1.0e-6]
mine_batch_size_schedule:
  - [0, 256]
  - [80_000, 128]
mine_smoothing_factor: 1  # MI = mine_smoothing_factor * current MI + (1-mine_smoothing_factor) * previous MI.
mine_weight_factor: 0.1   # loss = tts loss + mine_weight_factor * MI

# LOSSES
stop_loss_scaling: 8

# TRAINING
dropout_rate: 0.1
decoder_prenet_dropout_schedule:
  - [0, 0.]
  - [25_000, 0.]
  - [35_000, .5]
learning_rate_tts_schedule:
  - [0, 1.0e-4]
head_drop_schedule:  # head-level dropout: how many heads to set to zero at training time
  - [0, 0]
  - [15_000, 1]
reduction_factor_schedule:
  - [0, 10]
  - [80_000, 1]
max_steps: 900_000
tts_batch_size: 8
debug: False
with_stress: False

## Test LOGGING
#prediction_frequency: 50
#weights_save_frequency: 10
#train_images_plotting_frequency: 5
#keep_n_weights: 2
#keep_checkpoint_every_n_hours: 12
#n_steps_avg_losses: [100, 500, 1_000, 5_000]  # command line display of average loss values for the last n steps
#n_predictions: 1  # autoregressive predictions take time
#prediction_start_step: 50
#audio_start_step: 50
#audio_prediction_frequency: 50 # converting to glim takes time

# LOGGING
prediction_frequency: 10_000
weights_save_frequency: 10_000
train_images_plotting_frequency: 5_000
keep_n_weights: 2
keep_checkpoint_every_n_hours: 12
n_steps_avg_losses: [100, 500, 1_000, 5_000]  # command line display of average loss values for the last n steps
n_predictions: 1  # autoregressive predictions take time
prediction_start_step: 20_000
audio_start_step: 100_000
audio_prediction_frequency: 10_000 # converting to glim takes time
